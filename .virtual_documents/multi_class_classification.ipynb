import pandas as pd
import numpy as np


df = pd.read_csv("iris_synthetic_data.csv")
df.sample(5)


df.info()


df.describe()


df.isnull().sum()


X = df.iloc[:,:-1]
X


y = df.iloc[:,-1]
y


label_to_index = {label: idx for idx, label in enumerate(np.unique(y))}
index_to_label = { idx: label for label, idx in label_to_index.items()}

y = np.array([label_to_index[label] for label in y])
num_classes = len(label_to_index)
y[]


def one_hot_encoding(y, num_classes):
    one_hot_labels = np.zeros((y.size, num_classes))
    one_hot_labels[np.arange(y.size),y]=1
    return one_hot_labels

y_encoded = one_hot_encoding(y, num_classes)
y_encoded


from sklearn.model_selection import train_test_split 
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1764, random_state=42, stratify=y_train_val )


X_train, y_train


X_val, y_val


X_test, y_test


from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)
X_train, X_val, X_test


class multi_class_classification:
    def __innit__(self, alpha = 0.001, n_iteration = 50):
        self.alpha = alpha
        self.n_iteration = n_iteration
        self.weight = none
        self.bias = none
        self.losses = []

    def softmax(self, z):
        exp_z = np.exp(z-max(z))
        return exp_z/np.sum(exp_z)


mcc = multi_class_classification()
z=5
mcc.softmax(z)



